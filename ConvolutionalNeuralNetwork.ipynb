{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Character Recognizing Neural Network\n",
    "## Preprocessing"
   ],
   "id": "fbbc2398dee61f56"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Import Libraries\n",
    "- OS(os)\n",
    "- NumPy(numpy)\n",
    "    - NumPy Arrays\n",
    "- OpenCV 2(cv2)\n",
    "    - Load and process images\n",
    "- TensorFlow(tensorflow)\n",
    "    - Build network\n",
    "Optional:\n",
    "- MatPlotLib(matplotlib)\n",
    "    - Visualization"
   ],
   "id": "aa46dbec373a386"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T09:17:17.713295Z",
     "start_time": "2025-12-21T09:17:17.698827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import gzip\n",
    "import struct\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "16c758004a8b3331",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mcv2\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mgzip\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mstruct\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Helper Functions to Load EMNIST from Raw Files\n",
    "- Define function that loads .idx format images from the gzip file\n",
    "    - Takes parameter path which is the filename\n",
    "    - Open .gz file in binary read mode(\"rb\" parameter in gzip.open() function) and alias it as f\n",
    "        - open() automatically unzips file\n",
    "    - Method 1: Use struct to unpack the first 16 bytes of f into a tuple containing each piece of information from the header\n",
    "        - \">IIII\" argument tells unpack() to look in 4 byte(unsigned 32-bit integer) chunks in big-endian(>) order\n",
    "        - f.read(16) tells unpack() to look through first 16 bytes\n",
    "    - Store the second through fourth elements of the array in variables\n",
    "    - Method 2: Use struct to unpack directly into variables and skip first four bytes with _\n",
    "        - Does same thing as taking values from list, but more efficiently and more commonly used\n",
    "    - Read and store all remaining bytes from the file with f.read()\n",
    "        - Returns a bytes object to pass to np.frombuffer() which will turn the bytes buffer into a NumPy array\n",
    "            - Also pass dtype = np.uint8 to tell NumPy that each byte is one pixel with a value between 0 and 255\n",
    "            - frombuffer() wraps the bytes directly, it doesn't create new data\n",
    "    - Return the reshaped data\n",
    "        -  reshape() changes the 1D list(returned after converting from binary) into a 3D structure\n",
    "    - Do same for label data, but image byte conversion not necessary\n",
    "        - Also, data in chunks of 2 bytes, and only read first 8 bytes"
   ],
   "id": "f194314727b3b839"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T09:17:17.722241Z",
     "start_time": "2025-12-07T04:05:05.108508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_images(path):\n",
    "    with gzip.open(path, \"rb\") as f:\n",
    "        _, num_images, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        data = np.frombuffer(f.read(), dtype = np.uint8)\n",
    "        return data.reshape(num_images, rows, cols)\n",
    "\n",
    "def load_labels(path):\n",
    "    with gzip.open(path, \"rb\") as f:\n",
    "        _, num = struct.unpack(\">II\", f.read(8))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8)"
   ],
   "id": "14ca9cad9eef1dea",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Load EMNIST Data\n",
    "- Use helper functions accordingly to convert and store binary data sets as images"
   ],
   "id": "f5d8e8e70b520a0b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T09:17:17.727176Z",
     "start_time": "2025-12-07T04:05:05.114320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_train = load_images(\"gzip/emnist-balanced-train-images-idx3-ubyte.gz\")\n",
    "y_train = load_labels(\"gzip/emnist-balanced-train-labels-idx1-ubyte.gz\")\n",
    "\n",
    "x_test  = load_images(\"gzip/emnist-balanced-test-images-idx3-ubyte.gz\")\n",
    "y_test  = load_labels(\"gzip/emnist-balanced-test-labels-idx1-ubyte.gz\")"
   ],
   "id": "495e4a17885d1c4",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Use NumPy to Orient Images\n",
    "- Images are provided vertically flipped and rotated clockwise\n",
    "    - Use NumPy to rotate counterclockwise with np.rot90()\n",
    "        - Parameter x refers to dataset\n",
    "        - k = [number] refers to amount of 90º rotations\n",
    "        - axes(number, number) tells NumPy which two axes to rotate\n",
    "            - Since x has shape (images, rows, cols), axis refers to each individual image, axis 1 refers to rows, axis 2 refers to columns\n",
    "                - Works by transposing matrix along first axis, then flipping along second\n",
    "    - Use NumPy to flip image across vertical axis\n",
    "    - Do for both testing and training data"
   ],
   "id": "3c1e4de87d41cde4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T09:17:17.727490Z",
     "start_time": "2025-12-07T04:05:05.301548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_train = np.rot90(x_train, k = 1, axes = (1, 2))\n",
    "x_train = np.fliplr(x_train)\n",
    "x_test = np.rot90(x_test, k = 1, axes = (1, 2))\n",
    "x_test = np.fliplr(x_test)"
   ],
   "id": "3511ba7170e04642",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Normalize Data and Reduce Input\n",
    "- Scale everything to be between 0-1 instead of 0-255\n",
    "    - MNIST/EMNIST images have pixels stored as uint8 which are from 0(black) to 255(white) and middle values are gray\n",
    "        - Dividing by 255 keep them relatively the same color, and improves the neural network since:\n",
    "            - Scaling values in matrix multiplication with values 0-255 causes values to explode rapidly, vs they are more contained with values 0-1\n",
    "            - Activation functions will get saturated at extremes since input values will be giant if not scaled down\n",
    "        - Normalizing improves training speed, accuracy, and helps the functions behave more stably\n",
    "    - Have to do this for almost entirely mathematical reasons\n"
   ],
   "id": "55021fb012b7856"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T09:17:17.727898Z",
     "start_time": "2025-12-07T04:05:05.305864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_train = x_train/255.0\n",
    "x_test = x_test/255.0"
   ],
   "id": "9293a66b23f607e",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create a CNN\n",
    "- Create a Sequential Model object\n",
    "    - Keras model type where layers are put in a list, and data flows from start to finish\n",
    "    - Exactly one input and exactly one output\n",
    "    - model is an instance of Sequential which is a subclass of Model, and it keeps a list of layers, manages weights, runs forward passes, and knows how to train\n",
    "- Add 2D Convolutional Layer(trainable)\n",
    "    - Keras reshapes 3D image into 4D tensor implicitly\n",
    "    - Designed for image-like data\n",
    "    - Pass number of filters - 32\n",
    "        - The layer will learn 32 separate 3x3 convolution kernels meaning that each kernel scans the input image and produces one feature map\n",
    "        - Outputs 32 channels\n",
    "    - Pass kernel size - (3, 3)\n",
    "        - Each filter is a 3$\\times$3 grid of weights + a bias\n",
    "            - Does a 3x3 dot product with local patch in image for each position\n",
    "    - Set activation - 'relu'\n",
    "        - Sets negative responses to 0 and keeps other values the same\n",
    "    - Pass input shape - (28, 28, 1)\n",
    "        - Tells Keras that the input is of shape (height = 28, width = 28, channels = 1)\n",
    "                - Channel 1 means grayscale\n",
    "    - Layer will put a 3x3 window over the image for each filter and compute a weighted sum to see how much the slide matches up, then applies ReLU\n",
    "        - Converts image from 28$\\times$28 to 26$\\times$26\n",
    "- Add MaxPooling2D Layer (non-trainable)\n",
    "    - Downsample dimensions to keep only the strongest activations\n",
    "    - Set pool_size - (2, 2)\n",
    "        - Looks at non-overlapping 2x2 blocks and only takes the strongest activation from that block\n",
    "            - Since it takes one pixel from 2x2 blocks, the size of the input reduces by half, and output becomes of shape (batch_size, 13, 13, 32)\n",
    "            - Makes network more resistant to slight variations in handwriting, and helps it look for more general shape rather than get hung up on tiny differences\n",
    "- Repeat above steps again for second layer but don't provide an input shape\n",
    "    - Reduces output shape further, generalizing image more\n",
    "- Add Flatten() layer (non-trainable)\n",
    "    - Reshapes 4D tensor from previous layers into a 1D feature vector, making the output shape (batch_size, height * width * channels)\n",
    "    - Takes principle from Digit recognizing network so now the network can classify into predictions\n",
    "- Add Dense() layer(trainable)\n",
    "    - Add 128 Neurons and use ReLU activation\n",
    "    - Learns combinations of strokes and parts and uses this to distinguish characters\n",
    "        - Basically learns to combines pieces of characters to build the full character, and learns which combinations correspond to which character - most human-like learning\n",
    "- Add Dense() layer(trainable, final prediction layer)\n",
    "    - Give 47 Neurons for each possible class, and activate with softmax for probabilities\n",
    "        - Actually makes predictions"
   ],
   "id": "d4cefb8a155d0fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T09:17:17.728033Z",
     "start_time": "2025-12-07T04:05:05.433816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(47, activation='softmax')\n",
    "])"
   ],
   "id": "c0c5cae9e3420749",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Compile Model\n",
    "- Choose optimizer(usually 'adam')\n",
    "    - Adam is an advanced version of SGD which still uses mini-batching and gradient descent, just in a more optimizied way than pure mini-batching/pure sgd\n",
    "- Choose loss function(for this example, 'sparse_categorical_crossentropy'\n",
    "    -   Calculates cost/loss with -log($p_{y}$) where p is the model's probability and y is the expected probability(Usually 1)\n",
    "        - Works since log tends to -$\\infty$ faster as x approaches 0 than when x approaches $\\infty$\n",
    "            - Softmax squishes into 0-1, so when it's 0, the activation value needs to change by infinity(a lot) and when it's 1, the value doesn't need to change(definition of -log(x))\n",
    "- Choose metrics to monitor in evaluate(for this example, 'accuracy')"
   ],
   "id": "6080dec58b1cb469"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T09:17:17.729222Z",
     "start_time": "2025-12-07T04:05:05.459241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(x_train, y_train, epochs = 5)"
   ],
   "id": "29ae31aec5fc1ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001B[1m3525/3525\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m18s\u001B[0m 5ms/step - accuracy: 0.7944 - loss: 0.6462\n",
      "Epoch 2/5\n",
      "\u001B[1m3525/3525\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 5ms/step - accuracy: 0.8654 - loss: 0.3828\n",
      "Epoch 3/5\n",
      "\u001B[1m3525/3525\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 5ms/step - accuracy: 0.8818 - loss: 0.3243\n",
      "Epoch 4/5\n",
      "\u001B[1m3525/3525\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m19s\u001B[0m 5ms/step - accuracy: 0.8911 - loss: 0.2895\n",
      "Epoch 5/5\n",
      "\u001B[1m3525/3525\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m20s\u001B[0m 6ms/step - accuracy: 0.9009 - loss: 0.2604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x11880d0c0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Save and Load Model\n",
   "id": "c3cc311eaadc4d5c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T09:17:17.729602Z",
     "start_time": "2025-12-07T04:06:36.776846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.save('handwritten_characters.keras')\n",
    "model = tf.keras.models.load_model('handwritten_characters.keras')"
   ],
   "id": "713768a92bf15841",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Monitoring\n",
    "- Print how model is updating loss and cost\n",
    "- evaluate() returns cost/lost and other metrics provided in the compile function\n",
    "    - Pass the testing dataframes to model.evaluate()"
   ],
   "id": "2d474e12b4b95ca6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T09:17:17.729830Z",
     "start_time": "2025-12-07T04:06:36.851791Z"
    }
   },
   "cell_type": "code",
   "source": "loss, accuracy = model.evaluate(x_test, y_test)",
   "id": "be5dd5c3a763dbee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m588/588\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step - accuracy: 0.8770 - loss: 0.3649\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Testing with Custom Images\n",
    "- Set image_number to 0 since the first file is Digit0\n",
    "- loops through the current directory while a file Character{image_number} exists\n",
    "- imread() searches for a file titled Digit{number} and returns a NumPy array of shape (height, width, color in BGR)\n",
    "    - Use NumPy slicing to change it to only have one color instead of all 3 with [:, :, 0], selecting all columns, all rows, and the color blue since channel 0 in BGR corresponds to blue\n",
    "        - This is because you have to invert the array eventually since predict() takes an array of the shape (1, height, width), so you need to turn the 3 channels of color into one channel\n",
    "- Use OpenCV resize() to resize the image to 28x28 pixels since that's the input the model takes\n",
    "    - Interpolation is what adjusts the pixel brightness\n",
    "- Use NumPy invert method to invert the array so it matches the input shape requirement of predict()\n",
    "    - Then reshape into a 4D format since CNN outputs a 3D network\n",
    "- Make a prediction about the image with predict()\n",
    "- Use MatPlotLib method imshow() and plt.show() to show the image being predicted\n",
    "    - imshow() takes in the image as a parameter and applies a grayscale colormap which shows exactly what version of the image the model will process"
   ],
   "id": "4e2049e3d751a205"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T09:17:17.730015Z",
     "start_time": "2025-12-07T04:16:13.529729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "image_number = 1\n",
    "emnist_map = [\n",
    "    '0','1','2','3','4','5','6','7','8','9',\n",
    "    'A','B','C','D','E','F','G','H','I','J',\n",
    "    'K','L','M','N','O','P','Q','R','S','T',\n",
    "    'U','V','W','X','Y','Z',\n",
    "    'a','b','d','e','f','g','h','n','q','r','t'\n",
    "]\n",
    "while os.path.isfile(f\"Character{image_number}.png\"):\n",
    "    try:\n",
    "        img = cv2.imread(f\"Character{image_number}.png\")[:,:,0]\n",
    "        img = cv2.resize(img, (28, 28), interpolation=cv2.INTER_AREA)\n",
    "        img = np.invert(img)\n",
    "        img = img.reshape(1, 28, 28, 1)   # batch = 1 for one image, height, width, channels\n",
    "        prediction = model.predict(img)\n",
    "        plt.imshow(img[0], cmap = plt.cm.binary)\n",
    "        plt.show()\n",
    "        class_index = np.argmax(prediction)\n",
    "        print(f\"The character is probably: {emnist_map[class_index]}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"File wasn't found\")\n",
    "    except IndexError:\n",
    "        print(\"Index out of range\")\n",
    "    except:\n",
    "        print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "    finally: image_number += 1"
   ],
   "id": "fdc848cbc59987ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The character is probably: A\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Edge Cases",
   "id": "3f5f22b3747e18ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T09:17:17.730172Z",
     "start_time": "2025-12-07T04:06:38.110137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "image_number = 0\n",
    "while os.path.isfile(f\"Character_{image_number}.png\"):\n",
    "    try:\n",
    "        img = cv2.imread(f\"Character_{image_number}.png\")[:,:,0]\n",
    "        img = cv2.resize(img, (28, 28), interpolation=cv2.INTER_AREA)\n",
    "        img = np.invert(img)\n",
    "        img = img.reshape(1, 28, 28, 1)   # batch = 1 for one image, height, width, channels\n",
    "        prediction = model.predict(img)\n",
    "        plt.imshow(img[0], cmap = plt.cm.binary)\n",
    "        plt.show()\n",
    "        print(f\"The character is probably: {np.argmax(prediction)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"File wasn't found\")\n",
    "    except IndexError:\n",
    "        print(\"Index out of range\")\n",
    "    except:\n",
    "        print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "    finally: image_number += 1"
   ],
   "id": "32857a91ec646e18",
   "outputs": [],
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
